{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages (yeah idk )\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression # need\n",
    "from sklearn.model_selection import train_test_split # need\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler # need"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "# import alleged data set \n",
    "\n",
    "df = pd.read_csv(\"huh.csv\")\n",
    "\n",
    "#seperating our target and predictor \n",
    "X_rfr = df['predictor1', 'predictor2', 'predictor3']\n",
    "y_rfr = df['target']\n",
    "\n",
    "# make train test splits\n",
    "X_rfr_train, X_rfr_test, y_rfr_train, y_rfr_test = train_test_split(X_rfr, y_rfr, test_size=0.1, random_state = 42)\n",
    "\n",
    "# making things scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_rfr_train)\n",
    "\n",
    "X_rfr_train_sc = scaler.transform(X_rfr_train)\n",
    "X_rfr_test_sc = scaler.transform(X_rfr_test)\n",
    "\n",
    "# create an object of the RandomForestRegressor\n",
    "model_RFR = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_RFR.fit(X_rfr_train_sc, y_rfr_train)\n",
    "\n",
    "# predict the target on train and test data\n",
    "predict_train = model_RFR.predict(X_rfr_train_sc)\n",
    "predict_test = model_RFR.predict(X_rfr_test_sc)\n",
    "\n",
    "# Root Mean Squared Error on train and test data\n",
    "print('RMSE on train data: ', metrics.mean_squared_error(y_rfr_train, predict_train)**(0.5))\n",
    "print('RMSE on test data: ',  metrics.mean_squared_error(y_rfr_test, predict_test)**(0.5))\n",
    "\n",
    "r2 = metrics.r2_score(y_rfr_test, predict_test)\n",
    "print(r2)\n",
    "\n",
    "# plot to show feature importances\n",
    "plt.figure(figsize=(10,7))\n",
    "feat_importances = pd.Series(model_RFR.feature_importances_, index = X_rfr_train.columns)\n",
    "feat_importances.nlargest(7).plot(kind='barh');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "\n",
    "What if the relationship is linear ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# import dataset \n",
    "df = pd.read_csv(\"huh.csv\")\n",
    "\n",
    "# seperating our target and predictor\n",
    "X = df['predictor1', 'predictor2', 'predictor3'].to_numpy()\n",
    "y = df['target'].to_numpy()\n",
    "\n",
    "# seperating test train set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_sc, y_train)\n",
    "y_pred = lm.predict(X_test_sc)\n",
    "\n",
    "\n",
    "print('RMSE on test data: ',  metrics.mean_squared_error(y_test, y_pred)**(0.5))\n",
    "print('R^2:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "for funzies !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#define RMSE as a function, since we'll use this in the NN model \n",
    "def rmse(target,prediction):\n",
    "    return(np.sqrt(((target - prediction)**2).sum()/len(target)))\n",
    "\n",
    "\n",
    "#import data\n",
    "df = pd.read_csv('huh.csv')\n",
    "\n",
    "# normalize data\n",
    "df_norm = (data - data.mean())/data.std()\n",
    "df_norm.tail()\n",
    "\n",
    "#unpack variables\n",
    "target = df_norm['target']\n",
    "p2 = df_norm['p2']\n",
    "p3 = df_norm['p3']\n",
    "p4 = df_norm['p4']\n",
    "p5 = df_norm['p5']\n",
    "\n",
    "#target variable: y; predictor variable(s): x\n",
    "y = target\n",
    "y-=np.min(target)\n",
    "y/=np.max(target) #now y ranges from 0 to 1\n",
    "x = data_norm.drop('target',axis=1)\n",
    "\n",
    "#first do PCA, then use PCs as predictors\n",
    "n_modes = np.min(np.shape(x))\n",
    "pca = PCA(n_components = n_modes)\n",
    "PCs = pca.fit_transform(x)\n",
    "eigvecs = pca.components_\n",
    "fracVar = pca.explained_variance_ratio_\n",
    "\n",
    "#plot fraction of variance explained by each mode\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.scatter(range(len(fracVar)),fracVar)\n",
    "plt.xlabel('Mode Number')\n",
    "plt.ylabel('Fraction Variance Explained')\n",
    "plt.title('Variance Explained by All Modes')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#take first 5 PCs in MLP NN model\n",
    "#training: first 100 points\n",
    "#testing: remaining points\n",
    "\n",
    "ntrain = 100\n",
    "\n",
    "x_train = PCs[:ntrain,:5] #train on 100 observations of first 5 PCs\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "x_test = PCs[ntrain:,:5] #test on remaining observations\n",
    "y_test = y[ntrain:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OH GOD\n",
    "\n",
    "#This chunk of code is relatively flexible and can be used to loop through a range of parameters\n",
    "#We'll use this code several different times in the notebook\n",
    "#Play around and see what kinds of models you can make\n",
    "\n",
    "#First: MLP using 1 hidden layer with 10 neurons\n",
    "#Second: MLP using 1 hidden layer with 1-10 neurons\n",
    "#Third: something else!\n",
    "\n",
    "##### Play around with these parameters\n",
    "num_models = 10 #number of models to build for the ensemble\n",
    "min_nhn = 1 #minimum number of hidden neurons to loop through (nhn = 'number hidden neurons')\n",
    "max_nhn = 9 #maximum number of hidden neurons to loop through\n",
    "max_hidden_layers = 1 #maximum number of hidden layers to loop through (nhl = 'number hidden layers')\n",
    "batch_size = 32\n",
    "solver = 'adam' #use stochastic gradient descent as an optimization method (weight updating algorithm)\n",
    "activation = 'relu'\n",
    "learning_rate_init = 0.01\n",
    "#####\n",
    "\n",
    "max_iter = 1500 #max number of epochs to run\n",
    "early_stopping = True #True = stop early if validation error begins to rise\n",
    "validation_fraction = 0.1 #fraction of training data to use as validation\n",
    "\n",
    "y_out_all_nhn = []\n",
    "y_out_ensemble = []\n",
    "RMSE_ensemble = [] #RMSE for each model in the ensemble\n",
    "RMSE_ensemble_cumsum = [] #RMSE of the cumulative saltation for each model\n",
    "nhn_best = []\n",
    "nhl_best = []\n",
    "\n",
    "for model_num in range(num_models): #for each model in the ensemble\n",
    "    \n",
    "    print('Model Number: ' + str(model_num))\n",
    "    \n",
    "    RMSE = []\n",
    "    y_out_all_nhn = []\n",
    "    nhn = []\n",
    "    nhl = []\n",
    "    \n",
    "    for num_hidden_layers in range(1,max_hidden_layers+1):\n",
    "    \n",
    "        print('\\t # Hidden Layers = ' + str(num_hidden_layers))\n",
    "    \n",
    "        for num_hidden_neurons in range(min_nhn,max_nhn+1): #for each number of hidden neurons\n",
    "\n",
    "            print('\\t\\t # hidden neurons = ' + str(num_hidden_neurons))\n",
    "            \n",
    "            hidden_layer_sizes = (num_hidden_neurons,num_hidden_layers)\n",
    "            model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                                 verbose=False,\n",
    "                                 max_iter=max_iter, \n",
    "                                 early_stopping = early_stopping,\n",
    "                                 validation_fraction = validation_fraction,\n",
    "                                 batch_size = batch_size,\n",
    "                                 solver = solver,\n",
    "                                 activation = activation,\n",
    "                                 learning_rate_init = learning_rate_init)\n",
    "\n",
    "            model.fit(x_train,y_train) #train the model\n",
    "\n",
    "            y_out_this_nhn = model.predict(x_test) #model prediction for this number of hidden neurons (nhn)\n",
    "            y_out_all_nhn.append(y_out_this_nhn) #store all models -- will select best one best on RMSE\n",
    "\n",
    "            RMSE.append(rmse(y_test,y_out_this_nhn)) #RMSE between cumulative curves\n",
    "            \n",
    "            nhn.append(num_hidden_neurons)\n",
    "            nhl.append(num_hidden_layers)\n",
    "        \n",
    "    indBest = RMSE.index(np.min(RMSE)) #index of model with lowest RMSE\n",
    "    RMSE_ensemble.append(np.min(RMSE))\n",
    "    nhn_best.append(nhn[indBest])\n",
    "    nhl_best.append(nhl[indBest])\n",
    "    #nhn_best.append(indBest+1) #the number of hidden neurons that achieved best model performance of this model iteration\n",
    "    y_out_ensemble.append(y_out_all_nhn[indBest])\n",
    "    \n",
    "    print('\\t BEST: ' + str(nhl_best[model_num]) + ' hidden layers, '+ str(nhn_best[model_num]) + ' hidden neurons')\n",
    "    \n",
    "y_out_ensemble_mean = np.mean(y_out_ensemble,axis=0)\n",
    "RMSE_ensemble_mean = rmse(y_out_ensemble_mean,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualize\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(241)\n",
    "plt.scatter(len(RMSE_ensemble),RMSE_ensemble_mean,c='k',marker='*')\n",
    "plt.scatter(range(len(RMSE_ensemble)),RMSE_ensemble)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Error')\n",
    "\n",
    "plt.subplot(242)\n",
    "plt.scatter(range(len(nhn_best)),nhn_best)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('# Hidden Neurons')\n",
    "plt.title('Hidden Neurons')\n",
    "\n",
    "plt.subplot(243)\n",
    "plt.scatter(range(len(nhl_best)),nhl_best)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('# Hidden Layers')\n",
    "plt.title('Hidden Layers')\n",
    "\n",
    "plt.subplot(244)\n",
    "plt.scatter(y_test,y_out_ensemble_mean)\n",
    "#plt.plot((np.min(y_test),np.max(y_test)),'k--')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_model')\n",
    "plt.title('Ensemble')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(y_out_ensemble_mean)\n",
    "plt.plot(np.array(y_test),alpha = 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#visualize individual model runs\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12,5))\n",
    "\n",
    "plt.scatter(range(len(y_test)),y_test,label='Observations',zorder = 0,alpha = 0.3)\n",
    "plt.plot(range(len(y_test)),np.transpose(y_out_ensemble[0]),\n",
    "         color = 'k',alpha = 0.4,label='Individual Models',zorder=1) #plot first ensemble member with a label\n",
    "plt.plot(range(len(y_test)),np.transpose(y_out_ensemble[1:]),\n",
    "         color = 'k',alpha = 0.4,zorder=1) #plot remaining ensemble members without labels for a nicer legend\n",
    "plt.plot(range(len(y_test)),y_out_ensemble_mean,color = 'k',label = 'Ensemble',zorder=2, linewidth = 3)\n",
    "plt.xlabel('Time', fontsize = 20)\n",
    "plt.ylabel('y', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('MLP Model Results', fontsize = 24)\n",
    "plt.legend(fontsize = 16, loc = 'best')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#visualize performance metrics/etc\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(len(RMSE_ensemble),RMSE_ensemble_mean,c='k',marker='*', s = 150)\n",
    "plt.scatter(range(len(RMSE_ensemble)),RMSE_ensemble, s = 150)\n",
    "plt.xlabel('Model #', fontsize = 20)\n",
    "plt.ylabel('RMSE', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "#plt.ylim((np.min(RMSE_ensemble) - 0.005, np.max(RMSE_ensemble)+0.005))\n",
    "plt.title('Error', fontsize = 20)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(range(len(nhn_best)),nhn_best, s = 150)\n",
    "plt.xlabel('Model #', fontsize = 20)\n",
    "plt.ylabel('# Hidden Neurons', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Hidden Neurons', fontsize = 20)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(range(len(nhl_best)),nhl_best, s = 150)\n",
    "plt.xlabel('Model #', fontsize = 20)\n",
    "plt.ylabel('# Hidden Layers', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Hidden Layers', fontsize = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
